{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6DxGkPeQDD"
      },
      "source": [
        "***Challenge 1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfgxjTd_lk"
      },
      "source": [
        "Here the goal is to train on 25 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 25 from a datasource that is not the CIFAR-10 training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk0Ilt_-duk2"
      },
      "source": [
        "Feel free to modify this testbed to your liking, including the normalization transformations etc. Note however the final evaluation testbed will have a rigid set of components where you will need to place your answer. The only constraint is the data. Refer to the full project instructions for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyBTUe3idZI"
      },
      "source": [
        "Setup training functions. Again you are free to fully modify this testbed in your prototyping within the constraints of the data used. You can use tools outside of pytorch for training models if desired as well although the torchvision dataloaders will still be useful for interacting with the cifar-10 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "1IOj6O5og-r8"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4hpe7QbQFnr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SmallNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SmallNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 1024)\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(1024,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = x.unsqueeze(0)\n",
        "        x = self.pool1(nn.functional.relu(self.bn1(self.conv1(x))))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.pool2(nn.functional.relu(self.bn2(self.conv3(x))))\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return torch.nn.functional.sigmoid(x)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class RotationTransform:\n",
        "    def __init__(self, angle):\n",
        "        self.angle = angle\n",
        "\n",
        "    def __call__(self, x):\n",
        "        angle = math.radians(self.angle)\n",
        "        cos_theta = math.cos(angle)\n",
        "        sin_theta = math.sin(angle)\n",
        "        width, height = x.size\n",
        "        cx = width / 2\n",
        "        cy = height / 2\n",
        "        x_transformed = x.rotate(self.angle, resample=Image.BICUBIC)\n",
        "        x_transformed = transforms.functional.crop(x_transformed, cx - 16, cy - 16, 32, 32)\n",
        "        return x_transformed"
      ],
      "metadata": {
        "id": "T1ZWK83LabOb"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjWBE4MerTX"
      },
      "source": [
        "The below tries  2 random problem instances. In your development you may choose to prototype with 1 problem instances but keep in mind for small sample problems the variance is high so continously evaluating on several subsets will be important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v7xU1HMelJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548960a1-b148-4186-eb08-72cea7d55498"
      },
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# define transformation pipeline\n",
        "transform_train_rotate1 = transforms.Compose([\n",
        "    # transforms.ToPILImage(),\n",
        "    RotationTransform(-30),\n",
        "    # RotationTransform(30),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                        std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "# define transformation pipeline\n",
        "transform_train_rotate2 = transforms.Compose([\n",
        "    # transforms.ToPILImage(),\n",
        "    # RotationTransform(-30),\n",
        "    RotationTransform(30),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                        std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "# define transformation pipeline\n",
        "transform_train_horizontal = transforms.Compose([\n",
        "    # transforms.ToPILImage(),\n",
        "    # RotationTransform(-30),\n",
        "    # RotationTransform(30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                        std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "# define transformation pipeline\n",
        "transform_train_vertical = transforms.Compose([\n",
        "    # transforms.ToPILImage(),\n",
        "    # RotationTransform(-30),\n",
        "    RotationTransform(30),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                        std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(1, 5):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  train_data_augment1 = train_data\n",
        "  train_data_augment2 = train_data\n",
        "  train_data_augment3 = train_data\n",
        "  train_data_augment4 = train_data\n",
        "\n",
        "  train_data_augment1.transform = transform_train_rotate1\n",
        "  train_data_augment2.transform = transform_train_rotate2\n",
        "  train_data_augment3.transform = transform_train_horizontal\n",
        "  train_data_augment4.transform = transform_train_vertical\n",
        "\n",
        "  combined_dataset = torch.utils.data.ConcatDataset([train_data, train_data_augment1, train_data_augment2, train_data_augment3, train_data_augment4])\n",
        "  \n",
        "  combined_dataset_try = torch.utils.data.ConcatDataset([train_data, train_data_augment3])\n",
        "  # print('Num Samples For Training %d Num Samples For Val %d'%(combined_dataset.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(combined_dataset,\n",
        "                                             batch_size=64, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=64, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = SmallNet()\n",
        "  model.to(device)\n",
        "  epoch=9\n",
        "  optimizer = torch.optim.SGD(model.parameters(),lr=1e-6, momentum=0.9,\n",
        "                              weight_decay=5e-4)\n",
        "  for epoch in range(epoch):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 0 [174/250 (75%)]\tLoss: 2.271920\n",
            "Train Epoch: 5 [174/250 (75%)]\tLoss: 2.275716\n",
            "\n",
            "Test set: Average loss: 2.2879, Accuracy: 33/400 (8.25%)\n",
            "\n",
            "Train Epoch: 0 [174/250 (75%)]\tLoss: 2.301341\n",
            "Train Epoch: 5 [174/250 (75%)]\tLoss: 2.298358\n",
            "\n",
            "Test set: Average loss: 2.3027, Accuracy: 21/400 (5.25%)\n",
            "\n",
            "Train Epoch: 0 [174/250 (75%)]\tLoss: 2.274627\n",
            "Train Epoch: 5 [174/250 (75%)]\tLoss: 2.287519\n",
            "\n",
            "Test set: Average loss: 2.2860, Accuracy: 18/400 (4.50%)\n",
            "\n",
            "Train Epoch: 0 [174/250 (75%)]\tLoss: 2.313755\n",
            "Train Epoch: 5 [174/250 (75%)]\tLoss: 2.311782\n",
            "\n",
            "Test set: Average loss: 2.3071, Accuracy: 21/400 (5.25%)\n",
            "\n",
            "Acc over 5 instances: 5.81 +- 1.44\n"
          ]
        }
      ]
    }
  ]
}